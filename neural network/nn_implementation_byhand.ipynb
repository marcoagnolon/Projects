{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TTQA1a-pPZNe"
   },
   "source": [
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "ICT FOR LIFE AND HEALTH - Department of Information Engineering\n",
    "PHYSICS OF DATA - Department of Physics and Astronomy\n",
    "COGNITIVE NEUROSCIENCE AND CLINICAL NEUROPSYCHOLOGY - Department of Psychology\n",
    "\n",
    "A.A. 2019/20 (6 CFU)\n",
    "Dr. Alberto Testolin, Dr. Federico Chiariotti\n",
    "\n",
    "Author: Dr. Matteo Gadaleta\n",
    "\n",
    "Lab. 02 - Linear regression with artificial neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sjsnzZjPlj8"
   },
   "source": [
    "Define the true model and generate some noisy samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Chtc4oN3QdhH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "# Set random seed\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIAnk7pRu1jw"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3qHU_Lze_fL"
   },
   "outputs": [],
   "source": [
    "training_set=pd.read_csv('training_set.csv', header=None)\n",
    "test_set=pd.read_csv('test_set.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SUqX6aYQ3ih"
   },
   "outputs": [],
   "source": [
    "x_train = training_set[0].values\n",
    "y_train = training_set[1].values\n",
    "x_test = test_set[0]\n",
    "y_test = test_set[1]\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "\n",
    "mask=np.array(np.ones(len(x_train)), 'bool')\n",
    "x_cv=[]\n",
    "x_val=[]\n",
    "y_cv=[]\n",
    "y_val=[]\n",
    "\n",
    "for j in range(3):\n",
    "  temp_mask=mask.copy()\n",
    "  temp_mask[(j*40):((j+1)*40)]=False\n",
    "  x_cv.append(x_train[temp_mask])\n",
    "  x_val.append(x_train[~temp_mask])\n",
    "  y_cv.append(y_train[temp_mask])\n",
    "  y_val.append(y_train[~temp_mask])\n",
    "\n",
    "x_cv=np.array(x_cv)\n",
    "x_val=np.array(x_val)\n",
    "y_cv=np.array(y_cv)\n",
    "y_val=np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGVK9cBmRDUW"
   },
   "outputs": [],
   "source": [
    "### Plot\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_test, y_test, color='g', ls='', marker='.', label='Test data points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vumt0E-5RI1Q"
   },
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLfViys4RGkj"
   },
   "outputs": [],
   "source": [
    "# Define activation function\n",
    "from scipy.special import expit\n",
    "act_sig = expit\n",
    "# 1st derivative\n",
    "act_sig_der = lambda x: act_sig(x) * (1 - act_sig(x))\n",
    "\n",
    "# Plot activation function\n",
    "x_plot = np.linspace(-5,5,1000)\n",
    "y_act = act_sig(x_plot)\n",
    "y_act_der = act_sig_der(x_plot)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(x_plot, y_act, label='Sigmoid function')\n",
    "plt.plot(x_plot, y_act_der, label='Sigmoid 1st derivative')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzxcE2ZW7bsS"
   },
   "outputs": [],
   "source": [
    "act_tanh = lambda x: np.tanh(x)\n",
    "act_tanh_der = lambda x: 1/(np.cosh(x))**2\n",
    "\n",
    "# Plot activation function\n",
    "x_plot = np.linspace(-5,5,1000)\n",
    "y_act = act_tanh(x_plot)\n",
    "y_act_der = act_tanh_der(x_plot)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(x_plot, y_act, label='Tanh function')\n",
    "plt.plot(x_plot, y_act_der, label='Tanh 1st derivative')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CE4PX7Lgdvd"
   },
   "outputs": [],
   "source": [
    "# Define activation function\n",
    "act_relu= lambda x: x*(x>=0)+0*(x<0)\n",
    "act_relu_der = lambda x: 1*(x>=0)+0*(x<0)\n",
    "\n",
    "\n",
    "\n",
    "# Plot activation function\n",
    "x_plot = np.linspace(-5,5,1000)\n",
    "y_act = act_relu(x_plot)\n",
    "y_act_der = act_relu_der(x_plot)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(x_plot, y_act, label='ReLU function')\n",
    "plt.plot(x_plot, y_act_der, label='ReLU 1st derivative')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdEorDRXnEar"
   },
   "outputs": [],
   "source": [
    "\n",
    "act_leaky_relu= lambda x: x*(x>=0)+0.1*x*(x<0)\n",
    "act_leaky_relu_der = lambda x: 1*(x>=0)+0.1*(x<0)\n",
    "\n",
    "\n",
    "# Plot activation function\n",
    "x_plot = np.linspace(-5,5,1000)\n",
    "y_act = act_leaky_relu(x_plot)\n",
    "y_act_der = act_leaky_relu_der(x_plot)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(x_plot, y_act, label='Leaky ReLU function')\n",
    "plt.plot(x_plot, y_act_der, label='Leaky ReLU 1st derivative')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyv-r71u8N1k"
   },
   "outputs": [],
   "source": [
    "\n",
    "act_elu= lambda x: x*(x>=0)+0.5*(np.exp(x)-1)*(x<0)\n",
    "act_elu_der = lambda x: 1*(x>=0)+0.5*np.exp(x)*(x<0)\n",
    "\n",
    "\n",
    "# Plot activation function\n",
    "x_plot = np.linspace(-5,5,1000)\n",
    "y_act = act_elu(x_plot)\n",
    "y_act_der = act_elu_der(x_plot)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(x_plot, y_act, label='ELU function')\n",
    "plt.plot(x_plot, y_act_der, label='ELU 1st derivative')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D8q_7eWRPxG"
   },
   "source": [
    "##  Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agWZFciwPS4I"
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No, act_fn):\n",
    "            \n",
    "        ### WEIGHT INITIALIZATION (Xavier)\n",
    "        # Initialize hidden weights and biases (layer 1)\n",
    "        Wh1 = (np.random.rand(Nh1, Ni) - 0.5) * np.sqrt(12 / (Nh1 + Ni))\n",
    "        Bh1 = np.zeros([Nh1, 1])\n",
    "        self.WBh1 = np.concatenate([Wh1, Bh1], 1) # Weight matrix including biases\n",
    "        # Initialize hidden weights and biases (layer 2)\n",
    "        Wh2 = (np.random.rand(Nh2, Nh1) - 0.5) * np.sqrt(12 / (Nh2 + Nh1))\n",
    "        Bh2 = np.zeros([Nh2, 1])\n",
    "        self.WBh2 = np.concatenate([Wh2, Bh2], 1) # Weight matrix including biases\n",
    "        # Initialize output weights and biases\n",
    "        Wo = (np.random.rand(No, Nh2) - 0.5) * np.sqrt(12 / (No + Nh2))\n",
    "        Bo = np.zeros([No, 1])\n",
    "        self.WBo = np.concatenate([Wo, Bo], 1) # Weight matrix including biases\n",
    "\n",
    "        ### ACTIVATION FUNCTION\n",
    "\n",
    "        self.act_sig = expit\n",
    "        self.act_sig_der = lambda x: act_sig(x) * (1 - act_sig(x))\n",
    "        \n",
    "        self.act_tanh = lambda x: np.tanh(x)\n",
    "        self.act_tanh_der = lambda x: 1/(np.cosh(x))**2\n",
    "\n",
    "        self.act_relu= lambda x: x*(x>=0)+0*(x<0)\n",
    "        self.act_relu_der = lambda x: 1*(x>=0)+0*(x<0)\n",
    "\n",
    "        self.act_leaky_relu= lambda x: x*(x>=0)+0.1*(x<0)\n",
    "        self.act_leaky_relu_der = lambda x: 1*(x>=0)+0.1*(x<0)\n",
    "\n",
    "        self.act_elu= lambda x: x*(x>=0)+0.5*(np.exp(x)-1)*(x<0)\n",
    "        self.act_elu_der = lambda x: 1*(x>=0)+0.5*np.exp(x)*(x<0)\n",
    "\n",
    "        acts={'sigmoid':act_sig, 'tanh': act_tanh, 'ReLU': act_relu, 'Leaky ReLU': act_leaky_relu, 'ELU': act_elu}\n",
    "        acts_der={'sigmoid':act_sig_der, 'tanh': act_tanh_der, 'ReLU': act_relu_der, 'Leaky ReLU': act_leaky_relu_der, 'ELU': act_elu_der}\n",
    "        \n",
    "        self.act=acts[act_fn]\n",
    "        self.act_der=acts_der[act_fn]\n",
    "\n",
    "    #MAKE PREDICTIONS\n",
    "\n",
    "    def forward(self, x, additional_out=False):\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        x = np.array(x)\n",
    "        \n",
    "        ### Hidden layer 1\n",
    "        # Add bias term\n",
    "        X = np.append(x, 1)\n",
    "        # Forward pass (linear)\n",
    "        H1 = np.matmul(self.WBh1, X)\n",
    "        # Activation function\n",
    "        Z1 = self.act(H1)\n",
    "        \n",
    "        ### Hidden layer 2\n",
    "        # Add bias term\n",
    "        Z1 = np.append(Z1, 1)\n",
    "        # Forward pass (linear)\n",
    "        H2 = np.matmul(self.WBh2, Z1)\n",
    "        # Activation function\n",
    "        Z2 = self.act(H2)\n",
    "        \n",
    "        ### Output layer\n",
    "        # Add bias term\n",
    "        Z2 = np.append(Z2, 1)\n",
    "        # Forward pass (linear)\n",
    "        Y = np.matmul(self.WBo, Z2)\n",
    "        # NO activation function\n",
    "        \n",
    "        if additional_out:\n",
    "            return Y.squeeze(), Z2\n",
    "        \n",
    "        return Y.squeeze()\n",
    "        \n",
    "    #ADJUST WEIGHTS\n",
    "\n",
    "    def ADAMS(self, W, g, m, s, η, t, β1=0.9, β2=0.999, ε=10**(-8)):#, η=10**(-3)):\n",
    "        m = β1*m + (1-β1)*g\n",
    "        s = β2*s + (1-β2)*g**2\n",
    "        m_avg = m/(1-β1**(t+1))\n",
    "        s_avg = s/(1-β2**(t+1))\n",
    "        W = W - η*m_avg/((s_avg)**(1/2)+ε)\n",
    "        \n",
    "        return W, g, m, s\n",
    "\n",
    "    def update(self, x, label, η, num_ep):\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        X = np.array(x)\n",
    "        \n",
    "        ### Hidden layer 1\n",
    "        # Add bias term\n",
    "        X = np.append(X, 1)\n",
    "        # Forward pass (linear)\n",
    "        H1 = np.matmul(self.WBh1, X)\n",
    "        # Activation function\n",
    "        Z1 = self.act(H1)\n",
    "        \n",
    "        ### Hidden layer 2\n",
    "        # Add bias term\n",
    "        Z1 = np.append(Z1, 1)\n",
    "        # Forward pass (linear)\n",
    "        H2 = np.matmul(self.WBh2, Z1)\n",
    "        # Activation function\n",
    "        Z2 = self.act(H2)\n",
    "        \n",
    "        ### Output layer\n",
    "        # Add bias term\n",
    "        Z2 = np.append(Z2, 1)\n",
    "        # Forward pass (linear)\n",
    "        Y = np.matmul(self.WBo, Z2)\n",
    "        # NO activation function\n",
    "        \n",
    "        # Evaluate the derivative terms\n",
    "        D1 = Y - label \n",
    "        D2 = Z2\n",
    "        D3 = self.WBo[:,:-1]\n",
    "        D4 = self.act_der(H2)\n",
    "        D5 = Z1\n",
    "        D6 = self.WBh2[:,:-1]\n",
    "        D7 = self.act_der(H1)\n",
    "        D8 = X\n",
    "        \n",
    "        # Layer Error\n",
    "        Eo = D1\n",
    "        Eh2 = np.matmul(Eo, D3) * D4\n",
    "        Eh1 = np.matmul(Eh2, D6) * D7\n",
    "        \n",
    "        \n",
    "        # Derivative for weight matrices\n",
    "        dWBo = np.matmul(Eo.reshape(-1,1), D2.reshape(1,-1))\n",
    "        dWBh2 = np.matmul(Eh2.reshape(-1,1), D5.reshape(1,-1))\n",
    "        dWBh1 = np.matmul(Eh1.reshape(-1,1), D8.reshape(1,-1))\n",
    "        \n",
    "\n",
    "        # Update the weights\n",
    "\n",
    "        #we need global variables beacuse we need them to be initializated\n",
    "        #outside the class and then update at every call\n",
    "        global m1, s1, m2, s2, mo, so\n",
    "        \n",
    "        lambd=0  #L2 regularization\n",
    "        #updating with adams\n",
    "      \n",
    "        g1 = dWBh1+np.abs(self.WBh1)*lambd\n",
    "        WBh1, g1, m1, s1 = self.ADAMS(self.WBh1, g1, m1, s1,η, t=num_ep)\n",
    "\n",
    "        g2 = dWBh2+np.abs(self.WBh2)*lambd\n",
    "        self.WBh2, g2, m2, s2 = self.ADAMS(self.WBh2, g2, m2, s2,η, t=num_ep)\n",
    "\n",
    "        go = dWBo+np.abs(self.WBo)*lambd\n",
    "        self.WBo, go, mo, so = self.ADAMS(self.WBo, go, mo, so,η, t=num_ep)\n",
    "\n",
    "        #self.WBh1 -= lr * dWBh1\n",
    "        #self.WBh2 -= lr * dWBh2\n",
    "        #self.WBo -= lr * dWBo\n",
    "        \n",
    "        # Evaluate loss function\n",
    "        loss = (Y - label)**2/2\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def plot_weights(self):\n",
    "    \n",
    "        fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "        axs[0].hist(self.WBh1.flatten(), 20)\n",
    "        axs[1].hist(self.WBh2.flatten(), 50)\n",
    "        axs[2].hist(self.WBo.flatten(), 20)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRq8H2cOR5J2"
   },
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "Ni = 1 # Number of inputs\n",
    "Nh1 = 100# Number of hidden neurons (layer 1)\n",
    "Nh2 = 100# Number of hidden neurons (layer 2)\n",
    "No = 1 # Number of outputs\n",
    "act_fn = 'tanh' #activation function\n",
    "\n",
    "\n",
    "### Initialize network\n",
    "net = Network(Ni, Nh1, Nh2, No, act_fn)\n",
    "\n",
    "# Access the class members\n",
    "print('1st hidden layer weigth matrix shape:', net.WBh1.shape)\n",
    "print('2nd hidden layer weigth matrix shape:', net.WBh2.shape)\n",
    "print('Output layer weigth matrix shape:', net.WBo.shape)\n",
    "\n",
    "# Plot weights\n",
    "plt.close('all')\n",
    "net.plot_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7-7R2M-R8mQ"
   },
   "source": [
    "## FORWARD PASS (before training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzqBFqMERy7x"
   },
   "outputs": [],
   "source": [
    "# Define the x vector\n",
    "x_highres = np.linspace(min(x_train), max(x_train), 1000)\n",
    "# Evaluate the output for each input (this can be done as a batch, but for now let's do 1 input at a time)\n",
    "initial_net_output = []\n",
    "for x in x_highres:\n",
    "    net_out = net.forward(x)\n",
    "    initial_net_output.append(net_out)\n",
    "initial_net_output = np.array(initial_net_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Oqq1oY-SJcl"
   },
   "outputs": [],
   "source": [
    "# Or in just 1 line of pythonic code!!\n",
    "initial_net_output = np.array([net.forward(x) for x in x_highres])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLVBF9b8SHNB"
   },
   "outputs": [],
   "source": [
    "### Plot\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_highres, initial_net_output, color='g', ls='--', label='Network output (random weights)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlgEkjl9SMIH"
   },
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzAN4s0EoqZB"
   },
   "source": [
    "RANDOM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnj8K6HUwYGj"
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import choice\n",
    "from numpy.random import randint\n",
    "seed(1)\n",
    "\n",
    "N=15\n",
    "\n",
    "num_epochs = 5000\n",
    "η = 0.001\n",
    "en_decay = True\n",
    "η_final = 0.0005\n",
    "η_decay = (η_final / η)**(1 / num_epochs)\n",
    "\n",
    "Ni=1\n",
    "No=1\n",
    "\n",
    "act_fn_list = ['sigmoid', 'tanh', 'ReLU', 'Leaky ReLU', 'ELU']\n",
    "\n",
    "act_selection=[]\n",
    "for _ in range(N):\n",
    "    act_selection.append(choice(act_fn_list))\n",
    "\n",
    "\n",
    "Nh1_list = randint(10, 200, N)\n",
    "Nh2_list = randint(10, 200, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsQiOCY9hrk5"
   },
   "outputs": [],
   "source": [
    "print(Nh1_list)\n",
    "print(Nh2_list)\n",
    "print(act_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkdDtXKJ06Gd"
   },
   "outputs": [],
   "source": [
    "best_error= np.inf \n",
    "MSE=[]\n",
    "#create random search for the parameters\n",
    "\n",
    "for act_fn, Nh1, Nh2 in zip(act_selection, Nh1_list, Nh2_list):\n",
    "      print(act_fn)\n",
    "      print('Nh1', Nh1)\n",
    "      print('Nh2', Nh2)\n",
    "\n",
    "      \n",
    "      val_loss=0\n",
    "      stopping_index=0\n",
    "      cont=0\n",
    "\n",
    "      #training over k-folds\n",
    "      for xt,yt,xv,yv in zip(x_cv, y_cv, x_val, y_val):\n",
    "          print(cont)\n",
    "          cont+=1\n",
    "          \n",
    "          η = 0.001\n",
    "          η_final = 0.0005\n",
    "          η_decay = (η_final / η)**(1 / num_epochs)\n",
    "\n",
    "          #reilitialize the network for each training\n",
    "          net = Network(Ni, Nh1, Nh2, No, act_fn)\n",
    "          #train_loss_log=[]\n",
    "          val_loss_log=[]\n",
    "\n",
    "          #it needs to initialize this variables outside the cycle\n",
    "          m1= np.zeros(net.WBh1.shape)\n",
    "          s1= np.zeros(net.WBh1.shape)\n",
    "          m2= np.zeros(net.WBh2.shape)\n",
    "          s2= np.zeros(net.WBh2.shape)\n",
    "          mo= np.zeros(net.WBo.shape)\n",
    "          so= np.zeros(net.WBo.shape)\n",
    "\n",
    "\n",
    "          for num_ep in range(num_epochs):\n",
    "\n",
    "              # Learning rate decay\n",
    "              #if en_decay:\n",
    "              η *= η_decay\n",
    "\n",
    "              # Train single epoch (sample by sample, no batch for now)\n",
    "              train_loss_vec = [net.update(x, y,η, num_ep) for x, y in zip(x_train, y_train)]\n",
    "              #avg_train_loss = np.mean(train_loss_vec)\n",
    "              # val network\n",
    "              y_val_est = np.array([net.forward(x) for x in xv])\n",
    "              avg_val_loss = np.mean((y_val_est - yv)**2/2)\n",
    "\n",
    "              # Log\n",
    "              #train_loss_log.append(avg_train_loss)\n",
    "              val_loss_log.append(avg_val_loss)\n",
    "    \n",
    "              #print('Epoch %d - lr: %.5f - Train loss: %.5f - Val loss: %.5f' % (num_ep + 1, η, avg_train_loss, avg_val_loss))\n",
    "              \n",
    "              #early stopping condition GL\n",
    "              if (val_loss_log[-1]/min(val_loss_log)-1>0.04):\n",
    "                  break\n",
    "                \n",
    "          #average error between k-folds\n",
    "          val_loss+=min(val_loss_log)/len(x_cv)\n",
    "          if np.argmin(val_loss_log) > stopping_index:\n",
    "              stopping_index=np.argmin(val_loss_log)\n",
    "              \n",
    "      print('MSE:', val_loss)\n",
    "      print('index:', stopping_index)\n",
    "      MSE.append(val_loss)\n",
    "\n",
    "      #assign the best error to the lower value and choosing the parameters using it\n",
    "      if val_loss < best_error:\n",
    "            best_error = val_loss\n",
    "            best_Nh1=Nh1\n",
    "            best_Nh2=Nh2\n",
    "            best_act_fn=act_fn\n",
    "            best_epochs = stopping_index\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-oliuawovsS"
   },
   "source": [
    "BEST NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TLVZ8QJnapf"
   },
   "outputs": [],
   "source": [
    " print(best_Nh1, best_Nh2, best_act_fn, best_epochs, best_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oqTLkbQLMozQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYtodZljGX2X",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = best_epochs\n",
    "\n",
    "η = 0.001\n",
    "η_final = 0.0005\n",
    "η_decay = (η_final / η)**(1 / num_epochs)\n",
    "en_decay=True\n",
    "                          \n",
    "train_loss_log = []\n",
    "test_loss_log = []\n",
    "net = Network(Ni,best_Nh1,best_Nh2, No, best_act_fn)\n",
    "#it needs to initialize this variables outside the cycle\n",
    "m1= np.zeros(net.WBh1.shape)\n",
    "s1= np.zeros(net.WBh1.shape)\n",
    "m2= np.zeros(net.WBh2.shape)\n",
    "s2= np.zeros(net.WBh2.shape)\n",
    "mo= np.zeros(net.WBo.shape)\n",
    "so= np.zeros(net.WBo.shape)\n",
    "\n",
    "for num_ep in range(num_epochs):\n",
    "    # Learning rate decay\n",
    "    if en_decay:\n",
    "        η *= η_decay\n",
    "    # Train single epoch (sample by sample, no batch for now)\n",
    "    train_loss_vec = [net.update(x, y,η, num_ep) for x, y in zip(x_train, y_train)]\n",
    "    avg_train_loss = np.mean(train_loss_vec)\n",
    "    # Test network\n",
    "    y_test_est = np.array([net.forward(x) for x in x_test])\n",
    "    avg_test_loss = np.mean((y_test_est - y_test)**2/2)\n",
    "    # Log\n",
    "    train_loss_log.append(avg_train_loss)\n",
    "    test_loss_log.append(avg_test_loss)\n",
    "    print('Epoch %d  - Train loss: %.5f - Test loss: %.5f' % (num_ep + 1, avg_train_loss, avg_test_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5IUujRYRsS3"
   },
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(test_loss_log, label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNkfbUywRoxV"
   },
   "source": [
    "### Plot weights after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9PmgxzXRmET"
   },
   "outputs": [],
   "source": [
    "net.plot_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dyFsrD2hRhUc"
   },
   "source": [
    "###  FORWARD PASS (after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-C1YuYURdA4"
   },
   "outputs": [],
   "source": [
    "x_highres = np.linspace(min(x_train), max(x_train), 1000)\n",
    "net_output = np.array([net.forward(x) for x in x_highres])\n",
    "\n",
    "### Plot\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_test, y_test, color='b', ls='', marker='.', label='Test data points')\n",
    "plt.plot(x_highres, net_output, color='g', ls='--', label='Network output (trained weights)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rcf4-kK2RZjV"
   },
   "source": [
    "###  Analyze activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vGfvvHGdRXXe"
   },
   "outputs": [],
   "source": [
    "x1 = 0.1\n",
    "y1, z1 = net.forward(x1, additional_out=True)\n",
    "x2 = 2\n",
    "y2, z2 = net.forward(x2, additional_out=True)\n",
    "x3 = -3.5\n",
    "y3, z3 = net.forward(x3, additional_out=True)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "axs[0].stem(z1)\n",
    "axs[0].set_title('Last layer activations for input x=%.2f' % x1)\n",
    "axs[1].stem(z2)\n",
    "axs[1].set_title('Last layer activations for input x=%.2f' % x2)\n",
    "axs[2].stem(z3)\n",
    "axs[2].set_title('Last layer activations for input x=%.2f' % x3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CeRsBPr4qHTV"
   },
   "outputs": [],
   "source": [
    "np.save('WBh1', net.WBh1) \n",
    "np.save('WBh2', net.WBh2) \n",
    "np.save('WBo', net.WBo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KT2mJV7aEhNb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
